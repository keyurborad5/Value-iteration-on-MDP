{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving MDP using Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid world to setup my environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function:\n",
      "[[-8.22741238 -8.0304582   0.         -1.3906558  -0.434062    0.62882\n",
      "   1.8098      0.62882   ]\n",
      " [-8.0304582  -7.81162022  0.         -0.434062    0.62882     1.8098\n",
      "   3.122       1.8098    ]\n",
      " [-7.81162022 -7.56846691  0.         -1.3906558   0.          0.\n",
      "   4.58        3.122     ]\n",
      " [-7.56846691 -7.29829656  0.         -2.25159022 -3.0264312   0.\n",
      "   6.2         4.58      ]\n",
      " [-7.29829656 -6.99810729  0.         -3.0264312  -3.72378808  0.\n",
      "   8.          6.2       ]\n",
      " [-6.99810729 -6.66456366  0.          0.         -4.35140927  0.\n",
      "  10.          8.        ]\n",
      " [-6.66456366 -6.29395962 -5.88217736 -5.42464151 -4.91626834  0.\n",
      "   0.         10.        ]\n",
      " [-6.99810729 -6.66456366 -6.29395962 -5.88217736 -5.42464151  0.\n",
      "  10.          8.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, rows, cols, start, goal, obstacles):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.grid = np.full((rows, cols), -1)  # Default reward of -1 for all cells\n",
    "        self.grid[goal] = 10  # Reward of +10 for the goal cell\n",
    "        for obstacle in obstacles:\n",
    "            self.grid[obstacle] = -999  # None represents an obstacle\n",
    "\n",
    "    def is_valid_action(self, state, action):\n",
    "        \"\"\"Check if the action is valid from the given state.\"\"\"\n",
    "        row, col = state\n",
    "        if action == \"up\" and row > 0 and self.grid[row - 1, col] != -999:\n",
    "            return True\n",
    "        elif action == \"down\" and row < self.rows - 1 and self.grid[row + 1, col] != -999:\n",
    "            return True\n",
    "        elif action == \"left\" and col > 0 and self.grid[row, col - 1] != -999:\n",
    "            return True\n",
    "        elif action == \"right\" and col < self.cols - 1 and self.grid[row, col + 1] != -999:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"Get the next state given the current state and action.\"\"\"\n",
    "        row, col = state\n",
    "        if action == \"up\":\n",
    "            return (row - 1, col)\n",
    "        elif action == \"down\":\n",
    "            return (row + 1, col)\n",
    "        elif action == \"left\":\n",
    "            return (row, col - 1)\n",
    "        elif action == \"right\":\n",
    "            return (row, col + 1)\n",
    "        return state\n",
    "\n",
    "    def value_iteration(self, discount_factor=0.9, theta=0.001):\n",
    "        \"\"\"Perform value iteration to compute optimal policy.\"\"\"\n",
    "        values = np.zeros((self.rows, self.cols))  # Initialize value function to zeros\n",
    "        actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        while True:\n",
    "            delta = 0\n",
    "            new_values = np.copy(values)\n",
    "            \n",
    "            for row in range(self.rows):\n",
    "                for col in range(self.cols):\n",
    "                    if (row, col) in self.obstacles or (row, col) == self.goal:\n",
    "                        continue\n",
    "                    \n",
    "                    max_value = float('-inf')\n",
    "                    for action in actions:\n",
    "                        if not self.is_valid_action((row, col), action):\n",
    "                            continue\n",
    "                        \n",
    "                        next_state = self.get_next_state((row, col), action)\n",
    "                        reward = self.grid[next_state]\n",
    "                        max_value = max(max_value, reward + discount_factor * values[next_state])\n",
    "                    \n",
    "                    new_values[row][col] = max_value\n",
    "                    delta = max(delta, abs(new_values[row][col] - values[row][col]))\n",
    "            \n",
    "            values = new_values\n",
    "            \n",
    "            if delta < theta:  # Stop when values converge\n",
    "                break\n",
    "        \n",
    "        return values\n",
    "\n",
    "# Define grid world parameters\n",
    "rows = 8\n",
    "cols = 8\n",
    "start = (1, 1)\n",
    "goal = (6, 6)\n",
    "obstacles = [(0, 2),(1, 2),(2, 2),(3, 2),(4, 2),(5, 2), \n",
    "            (2, 5), (3, 5), (4, 5), (5, 5), (6, 5), (7, 5),\n",
    "             (5,3), \n",
    "             (2,4),]\n",
    "\n",
    "# Create grid world instance\n",
    "grid_world = GridWorld(rows=rows, cols=cols, start=start, goal=goal, obstacles=obstacles)\n",
    "\n",
    "# Perform value iteration\n",
    "optimal_values = grid_world.value_iteration()\n",
    "\n",
    "# Print results\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimal Value Function:\n",
    "[[-8.22741238 -8.0304582   0.         -1.3906558  -0.434062    0.62882  1.8098      0.62882   ]\n",
    " [-8.0304582  -7.81162022  0.         -0.434062    0.62882     1.8098   3.122       1.8098    ]\n",
    " [-7.81162022 -7.56846691  0.         -1.3906558   0.          0.       4.58        3.122     ]\n",
    " [-7.56846691 -7.29829656  0.         -2.25159022 -3.0264312   0.       6.2         4.58      ]\n",
    " [-7.29829656 -6.99810729  0.         -3.0264312  -3.72378808  0.       8.          6.2       ]\n",
    " [-6.99810729 -6.66456366  0.          0.         -4.35140927  0.      10.          8.        ]\n",
    " [-6.66456366 -6.29395962 -5.88217736 -5.42464151 -4.91626834  0.       0.         10.        ]\n",
    " [-6.99810729 -6.66456366 -6.29395962 -5.88217736 -5.42464151  0.      10.          8.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[-9.99140496 -9.99140496  0.         -1.3906558  -0.434062    0.62882  1.8098      0.62882   ]\n",
    " [-9.99140496 -9.99140496  0.         -0.434062    0.62882     1.8098   3.122       1.8098    ]\n",
    " [-9.99140496 -9.99140496  0.          0.          0.          0.       4.58        3.122     ]\n",
    " [-9.99140496 -9.99140496  0.         -9.99140496 -9.99140496  0.       6.2         4.58      ]\n",
    " [-9.99140496 -9.99140496  0.         -9.99140496 -9.99140496  0.       8.          6.2       ]\n",
    " [-9.99140496 -9.99140496  0.          0.          0.          0.      10.          8.        ]\n",
    " [-9.99140496 -9.99140496 -9.99140496 -9.99140496 -9.99140496  0.       0.         10.        ]\n",
    " [-9.99140496 -9.99140496 -9.99140496 -9.99140496 -9.99140496  0.      10.          8.        ]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
